{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ria1994maitra/Docs/blob/main/Clause_to_Tree_Triplets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6213cb9e",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "3e0e291143c04aa9868788a830e878d6",
            "c1b5c884d70a4a26b1b67f8f00df8480"
          ]
        },
        "id": "6213cb9e",
        "outputId": "a1e95c11-9068-4426-cc4d-ba4728e5bc5f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e0e291143c04aa9868788a830e878d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-10-04 16:00:18 INFO: Downloading default packages for language: en (English)...\n",
            "2022-10-04 16:00:20 INFO: File exists: C:\\Users\\MehulGupta\\stanza_resources\\en\\default.zip\n",
            "2022-10-04 16:00:25 INFO: Finished downloading models and saved to C:\\Users\\MehulGupta\\stanza_resources.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1b5c884d70a4a26b1b67f8f00df8480",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-10-04 16:00:26 INFO: Loading these models for language: en (English):\n",
            "===========================\n",
            "| Processor    | Package  |\n",
            "---------------------------\n",
            "| tokenize     | combined |\n",
            "| pos          | combined |\n",
            "| constituency | wsj      |\n",
            "===========================\n",
            "\n",
            "2022-10-04 16:00:26 INFO: Use device: cpu\n",
            "2022-10-04 16:00:26 INFO: Loading: tokenize\n",
            "2022-10-04 16:00:26 INFO: Loading: pos\n",
            "2022-10-04 16:00:26 INFO: Loading: constituency\n",
            "2022-10-04 16:00:27 INFO: Done loading processors!\n"
          ]
        }
      ],
      "source": [
        "import stanza\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "stanza.download('en')       # This downloads the English models for the neural pipeline\n",
        "nlp_stanza = stanza.Pipeline('en',processors=['tokenize','pos','constituency'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "029aa60d",
      "metadata": {
        "id": "029aa60d"
      },
      "outputs": [],
      "source": [
        "def iter_tree_stanza(sent, depth): #recursive DFS\n",
        "    a_inter = []\n",
        "    if (sent.is_preterminal() == False) or (sent is None):\n",
        "        for child in sent.children:\n",
        "            # if str(' '.join(child.leaf_labels())).strip() in (',','(',')','i','ii','iii'):\n",
        "            #     pass\n",
        "\n",
        "            # if child.label in ('CC'):\n",
        "            #   a_inter.append([depth , ' '.join(child.leaf_labels()) , child.label , '--'])\n",
        "\n",
        "            if sent.label in ('S','VP','PP') and child.label in ('NP',):\n",
        "                check = 0 \n",
        "\n",
        "                for gchild in child.children:\n",
        "                    if gchild.label in ('NP','VP','S'):\n",
        "                        check = 1 \n",
        "\n",
        "                if check == 1:\n",
        "                    for item in iter_tree_stanza(child, depth+1):\n",
        "                        a_inter.append(item)\n",
        "                else: \n",
        "                    a_inter.append([depth , ' '.join(child.leaf_labels()) , child.label , 'Node'])\n",
        "\n",
        "\n",
        "            elif sent.label in ('NP') and child.label in ('NP','NNP','NN','NML','NNS','JJ','VBN','ADJP'):\n",
        "                check = 0 \n",
        "\n",
        "                for gchild in child.children:\n",
        "                    if gchild.label in ('NP','VP','NML','NN'):\n",
        "                        check = 1 \n",
        "\n",
        "                if check == 1:\n",
        "                    for item in iter_tree_stanza(child, depth+1):\n",
        "                        a_inter.append(item)\n",
        "                else:\n",
        "                    a_inter.append([depth , ' '.join(child.leaf_labels()) , child.label , 'Node'])                      \n",
        "                                                        \n",
        "\n",
        "            elif sent.label in ('NML') and child.label in ('NN'):\n",
        "                a_inter.append([depth , ' '.join(child.leaf_labels()) , child.label , 'Node'])\n",
        "\n",
        "\n",
        "            elif sent.label in ('VP') and child.label in ('VBZ','MD','VB','VBG','VBN','NNS','RB'):\n",
        "\n",
        "                a_inter.append([depth , ' '.join(child.leaf_labels()) , child.label , 'Edge'])         \n",
        "\n",
        "\n",
        "            elif sent.label in ('PP') and child.label in ('IN'):\n",
        "                a_inter.append([depth , ' '.join(child.leaf_labels()) , child.label , 'Edge'])                                  \n",
        "          \n",
        "                    \n",
        "\n",
        "                                                                                                   \n",
        "    #####################\n",
        "    # New Rule added here - End \n",
        "    #####################                   \n",
        "            else:\n",
        "              for item in iter_tree_stanza(child, depth+1):\n",
        "                    a_inter.append(item)\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    return a_inter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f08bbd3f",
      "metadata": {
        "id": "f08bbd3f"
      },
      "outputs": [],
      "source": [
        "def components_to_tree(results):\n",
        "    # len_results = len(results)\n",
        "\n",
        "    # for elmnt_seq in range(len_results):\n",
        "    #     if results[elmnt_seq][2] == 'CC':\n",
        "    #         if results[elmnt_seq-1][3] == results[elmnt_seq+1][3]:\n",
        "    #             results[elmnt_seq][3] = results[elmnt_seq-1][3]\n",
        "\n",
        "    ### Combine successsive edge\n",
        "    elmnt_seq = 0\n",
        "    while elmnt_seq < len(results)-1:\n",
        "        if results[elmnt_seq][3] == 'Edge' and results[elmnt_seq+1][3] == 'Edge':\n",
        "            results[elmnt_seq][1] = results[elmnt_seq][1] + ' ' +  results[elmnt_seq+1][1]\n",
        "            results.remove(results[elmnt_seq+1])\n",
        "            elmnt_seq=elmnt_seq-1\n",
        "        else:\n",
        "            elmnt_seq = elmnt_seq + 1\n",
        "\n",
        "    ### Combine successsive Nouns / Adjectives\n",
        "    elmnt_seq = 0\n",
        "    while elmnt_seq < len(results)-1:\n",
        "        if (results[elmnt_seq][2] in ('NN','NNS') and results[elmnt_seq+1][2] in ('NN','NNS')) or (results[elmnt_seq][2] in ('JJ','JJR','JJS','ADJP') and results[elmnt_seq+1][2] in ('JJ','JJR','JJS','ADJP')):\n",
        "            results[elmnt_seq][1] = results[elmnt_seq][1] + ' ' +  results[elmnt_seq+1][1]\n",
        "            results.remove(results[elmnt_seq+1])\n",
        "            elmnt_seq=elmnt_seq-1\n",
        "        else:\n",
        "            elmnt_seq = elmnt_seq + 1     \n",
        "\n",
        "    ### Combine Adjectives to entity\n",
        "    elmnt_seq = 0\n",
        "    while elmnt_seq < len(results)-1:\n",
        "        if results[elmnt_seq][2] in ('JJ','JJR','JJS','ADJP'):\n",
        "            results[elmnt_seq][1] = results[elmnt_seq][1] + ' ' +  results[elmnt_seq+1][1]\n",
        "            results[elmnt_seq][2] = results[elmnt_seq+1][2]\n",
        "            results.remove(results[elmnt_seq+1])\n",
        "        elmnt_seq = elmnt_seq + 1    \n",
        "\n",
        "\n",
        "    ### Combine 's based strings\n",
        "    elmnt_seq = 0\n",
        "    while elmnt_seq < len(results)-1:\n",
        "        if \"'s\" in results[elmnt_seq][1]:\n",
        "            results[elmnt_seq][1] = results[elmnt_seq][1] + ' ' +  results[elmnt_seq+1][1]\n",
        "            results.remove(results[elmnt_seq+1])\n",
        "        elmnt_seq = elmnt_seq + 1     \n",
        "\n",
        "\n",
        "    ### Combine \"any\" , \"of\" and whatever is followed up entity \n",
        "    elmnt_seq = 0\n",
        "    while elmnt_seq < len(results)-1:\n",
        "        if results[elmnt_seq][1].strip().lower() == 'any' and results[elmnt_seq+1][1].strip().lower() == 'of':\n",
        "            results[elmnt_seq][1] = results[elmnt_seq][1] + ' ' +  results[elmnt_seq+1][1] + ' ' +  results[elmnt_seq+2][1]\n",
        "            results.remove(results[elmnt_seq+2])\n",
        "            results.remove(results[elmnt_seq+1])\n",
        "        elmnt_seq = elmnt_seq + 1 \n",
        "\n",
        "\n",
        "    ### Combine if starting is an edge, to create node\n",
        "    if results[0][3] == 'Edge':\n",
        "        new_node_val = results[0][1]\n",
        "        elmnt_seq = 1\n",
        "        while elmnt_seq < len(results)-1:\n",
        "            if results[elmnt_seq][3] == 'Node':\n",
        "                new_node_val = new_node_val + ' ' +  results[elmnt_seq][1]\n",
        "                results.remove(results[elmnt_seq])\n",
        "            else:\n",
        "                break\n",
        "        results[0][3] = 'Node'\n",
        "        results[0][1] = new_node_val\n",
        "\n",
        "    # return results\n",
        "\n",
        "    ### Convert result into tree\n",
        "    elmnt_seq = 0\n",
        "    prev_node = [results[elmnt_seq][1]]\n",
        "    prev_edge = results[elmnt_seq+1][1]\n",
        "    tree_triplets = []\n",
        "    current_node_series = []\n",
        "\n",
        "    for elmnt_seq in range(2,len(results)):\n",
        "\n",
        "        if results[elmnt_seq][3] == 'Edge':\n",
        "            prev_edge = results[elmnt_seq][1]\n",
        "            prev_node = current_node_series\n",
        "            current_node_series = []\n",
        "\n",
        "        else:\n",
        "            if elmnt_seq+1 < len(results):\n",
        "                if results[elmnt_seq+1][3] == 'Edge':\n",
        "\n",
        "                    if results[elmnt_seq-1][3] == 'Edge':\n",
        "                        for nodes in prev_node:\n",
        "                            tree_triplets.append([nodes,prev_edge,results[elmnt_seq][1]])\n",
        "                        current_node_series = [results[elmnt_seq][1]]\n",
        "                    \n",
        "                    else:\n",
        "                        for nodes in prev_node:\n",
        "                            tree_triplets.append([nodes,prev_edge,results[elmnt_seq][1]])\n",
        "                        current_node_series.append(results[elmnt_seq][1])\n",
        "            \n",
        "                else:\n",
        "                    if results[elmnt_seq-1][3] == 'Edge':\n",
        "                        for nodes in prev_node:\n",
        "                            tree_triplets.append([nodes,prev_edge,results[elmnt_seq][1]])\n",
        "                        current_node_series.append(results[elmnt_seq][1])\n",
        "                    \n",
        "                    else:\n",
        "                        for nodes in prev_node:\n",
        "                            tree_triplets.append([nodes,prev_edge,results[elmnt_seq][1]])\n",
        "                        current_node_series.append(results[elmnt_seq][1])\n",
        "\n",
        "            else:\n",
        "                for nodes in prev_node:\n",
        "                    tree_triplets.append([nodes,prev_edge,results[elmnt_seq][1]])\n",
        "\n",
        "\n",
        "\n",
        "    return tree_triplets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90f9b118",
      "metadata": {
        "id": "90f9b118",
        "outputId": "72dd53ca-b499-4fc8-dd93-331477448f36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Customer', 'agrees', 'CyberSource']\n",
            "['Customer', 'agrees', 'its Affiliates']\n",
            "['CyberSource', 'may access use', 'Personal Information']\n",
            "['its Affiliates', 'may access use', 'Personal Information']\n",
            "['Personal Information', 'for improving enhancing underlying products used by', 'Customer']\n",
            "['Customer', 'detecting', 'data security incidents']\n",
            "['data security incidents', 'improving enhancing', 'security fraud prevention tools']\n",
            "['security fraud prevention tools', 'for', 'use']\n",
            "['use', 'by', 'Customer']\n",
            "['use', 'by', 'any other customers']\n",
            "['use', 'by', 'clients']\n",
            "['Customer', 'of', 'CyberSource']\n",
            "['any other customers', 'of', 'CyberSource']\n",
            "['clients', 'of', 'CyberSource']\n",
            "['Customer', 'of', 'its Affiliates']\n",
            "['any other customers', 'of', 'its Affiliates']\n",
            "['clients', 'of', 'its Affiliates']\n",
            "\n",
            "\n",
            " ----------------------------- ----------------------------- ----------------------------- \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clause_list = [\n",
        "              \"Customer agrees that CyberSource and its Affiliates may access and use Personal Information for improving/enhancing underlying products used by Customer, detecting data security incidents, or improving and enhancing security and fraud prevention tools for use by Customer and/or any other customers and clients of CyberSource or its Affiliates\",\n",
        "              # \"Issuer hereby consents to Visa or its affiliate providing Cardinal with access to Issuer's fraud, chargeback and authentication- and authorization-level data for Visa transactions, solely to enable Cardinal to provide reporting to Issuer and for analyses and efforts to improve and enhance the fraud and risk capabilities of Cardinal's Service.\",\n",
        "              # \"Cardinal may, however, independently use anonymous and/or aggregated information for pusposes of improving its products and services in accordance with applicable law.\",\n",
        "              # \"CyberSource fruther represents and warrants that CyberSource will not use or disclose unique, non-public end-user data submitted by customer except as reasonably necessary (i) to provide the transaction services to customer hereunder, (ii) to provide fraud screen services generally without disclosing personally identifiable end-user information ,or, (iii) as otherwise permitted or required by law\",\n",
        "              # \"For clarity, the foregoing consent does not authorize Visa to sell merchant's data or disclose it to third parties without merchant's prior written consent.\",\n",
        "              # \"The rights, duties and/or obligations of CyberSource under this agreement may be exercised and/or performed by CyberSource and/or any of CyberSource's affiliates, or any of their subcontractors and/or agents.\",\n",
        "              ## Sample from online\n",
        "              # \"The employment relationship between the parties shall be governed by this Agreement and by the policies and practices established by the Company and/or its Board.\",\n",
        "              # \"SUBRECIPIENT shall also comply with all applicable parts of COUNTY’S WIOA Policies and Procedures for recruitment, intake, assessment and referral, copies of which are available from COUNTY’S PROJECT MANAGER.\",\n",
        "              ]\n",
        "\n",
        "\n",
        "for clause in clause_list:\n",
        "    doc3 = nlp_stanza(clause)\n",
        "    sent3 = doc3.sentences[0].constituency.children[0]\n",
        "    # print(sent3.label)\n",
        "    # print(' '.join(sent3.leaf_labels()))\n",
        "    results_stp1 = iter_tree_stanza(sent3, 0)\n",
        "\n",
        "    # for items in results_stp1:\n",
        "    #   print(items)\n",
        "\n",
        "    # print(\"\\n\\n ----------------------------- \\n\\n\")\n",
        "\n",
        "    results = components_to_tree(results_stp1)\n",
        "\n",
        "    for items in results:\n",
        "        print(items)\n",
        "\n",
        "    print(\"\\n\\n ----------------------------- ----------------------------- ----------------------------- \\n\\n\")          \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81da7eda",
      "metadata": {
        "id": "81da7eda"
      },
      "outputs": [],
      "source": [
        "\n",
        "edges = [[\"start\",results[0][0]]]\n",
        "\n",
        "for elements in results:\n",
        "    edges.append([elements[0],elements[2]])\n",
        "edge_lables = {(\"start\",results[0][0]):\"here\"}\n",
        "\n",
        "\n",
        "\n",
        "for elements in results:\n",
        "    edge_lables[(elements[0],elements[2])] = elements[1]\n",
        "\n",
        "\n",
        "G = nx.MultiDiGraph()\n",
        "G.add_edges_from(edges)\n",
        "\n",
        "color_map = []\n",
        "for node in G:\n",
        "    if node != \"start\":\n",
        "        color_map.append('pink')\n",
        "    else: \n",
        "        color_map.append(\"tab:blue\")\n",
        "\n",
        "pos = nx.spring_layout(G)\n",
        "plt.figure()\n",
        "nx.draw(\n",
        "    G, pos, edge_color='black', width=1, linewidths=1,\n",
        "    node_size=500, node_color=color_map, alpha=0.9,connectionstyle='arc3, rad = 0.1',\n",
        "    labels={node: node for node in G.nodes()}\n",
        ")\n",
        "nx.draw_networkx_edge_labels(\n",
        "    G, pos,\n",
        "    edge_labels=edge_lables,\n",
        "    font_color='red'\n",
        ")\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "plt.axis('off')\n",
        "# plt.figure(1,figsize=(1000,1000), dpi=1000)\n",
        "\n",
        "fig = plt.figure()\n",
        " \n",
        "fig.set_figheight(10)\n",
        "fig.set_figwidth(10)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aae0d458",
      "metadata": {
        "id": "aae0d458"
      },
      "outputs": [],
      "source": [
        "pdf_info.render_to()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pypdfium2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK7jRF5rOWRy",
        "outputId": "7d321cbc-cbbd-403b-a20b-357539b8fd29"
      },
      "id": "lK7jRF5rOWRy",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pypdfium2\n",
            "  Downloading pypdfium2-3.19.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2\n",
            "Successfully installed pypdfium2-3.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pypdfium2 as pdf"
      ],
      "metadata": {
        "id": "2BOguelWOZd5"
      },
      "id": "2BOguelWOZd5",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf.PdfDocument.render_to(pdf.BitmapConv.pil_image, page_indices , scale = )\n",
        "def render_to(converter, page_indices=None, n_processes=os.cpu_count(), **kwargs)\n",
        "\n",
        "render_to: (self: PdfDocument, converter: Any, page_indices: Any | None = None, n_processes: int | None = os.cpu_count(), **kwargs: Any) -> Any\n",
        "\n",
        "Concurrently render multiple pages, using a process pool executor.\n",
        "\n",
        "If rendering only a single page, the call is simply forwarded to .PdfPage.render_to as a shortcut.\n",
        "\n",
        "Parameters:\n",
        "    page_indices (typing.Sequence[int] | None):\n",
        "        A sequence of zero-based indices of the pages to render. Reverse indexing or duplicate page indices are prohibited.\n",
        "If None, all pages will be included. The order of results is guaranteed to match the order of given page indices.\n",
        "    n_processes (int):\n",
        "        Target number of parallel processes.\n",
        "    kwargs (dict):\n",
        "        Keyword arguments to the renderer. See .PdfPage.render_to / .PdfPage.render_base.\n",
        "\n",
        "Yields:\n",
        "    typing.Any: Implementation-specific result object.\n",
        "\n",
        "\n",
        "Get the bitmap as PIL image\n"
      ],
      "metadata": {
        "id": "jenHQ8DqQtO2"
      },
      "id": "jenHQ8DqQtO2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SPDX-FileCopyrightText: 2022 geisserml <geisserml@gmail.com>\n",
        "# SPDX-License-Identifier: Apache-2.0 OR BSD-3-Clause\n",
        "\n",
        "from pypdfium2._helpers.misc import BitmapStrReverseToRegular\n",
        "\n",
        "try:\n",
        "    import PIL.Image\n",
        "except ImportError:\n",
        "    PIL = None\n",
        "\n",
        "try:\n",
        "    import numpy.ctypeslib\n",
        "except ImportError:\n",
        "    numpy = None\n",
        "\n",
        "\n",
        "class BitmapConvBase:\n",
        "    \"\"\"\n",
        "    Parent class for bitmap converters compatible with :meth:`.PdfPage.render_to` / :meth:`.PdfDocument.render_to`.\n",
        "    The initialiser captures any arguments and adds them to the :meth:`.run` call.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.args = args\n",
        "        self.kwargs = kwargs\n",
        "    \n",
        "    @staticmethod\n",
        "    def run(result, renderer_kws, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        The actual converter function, to be implemented by the inheriting class.\n",
        "        \n",
        "        Parameters:\n",
        "            result (tuple):\n",
        "                Result of the :meth:`~.PdfPage.render_base` call (ctypes array, colour format, size).\n",
        "            renderer_kws (dict):\n",
        "                Dictionary of rendering keywords that were passed in by the caller.\n",
        "            args (tuple):\n",
        "                Further positional arguments to the converter, as captured by the initialiser.\n",
        "            kwargs (dict):\n",
        "                Further keyword arguments to the converter, as captured by the initialiser.\n",
        "        Returns:\n",
        "            typing.Any: The converted rendering result (implementation-specific).\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Inheriting class must provide run() method.\")\n",
        "\n",
        "\n",
        "class BitmapConv:\n",
        "    \"\"\"\n",
        "    Built-in converters to be applied on the rendering result.\n",
        "    \"\"\"\n",
        "    \n",
        "    class any (BitmapConvBase):\n",
        "        \"\"\"\n",
        "        Simple factory for converters that merely work with the ctypes array, while passing through additional information unaffected.\n",
        "        \n",
        "        Example:\n",
        "            ``render_to(BitmapConv.any(bytes), **kwargs)``:\n",
        "                Get an independent copy of the pixel data as bytes.\n",
        "        \n",
        "        Parameters:\n",
        "            converter (typing.Callable):\n",
        "                A function to translate a ctypes array to a different data type.\n",
        "        Returns:\n",
        "            (typing.Any, str, (int, int)): The converted bitmap (implementation-specific), and additional information returned by :meth:`~.PdfPage.render_base` (colour format, size).\n",
        "        \"\"\"\n",
        "        \n",
        "        @staticmethod\n",
        "        def run(result, renderer_kws, converter):\n",
        "            c_array, cl_format, size = result\n",
        "            return converter(c_array), cl_format, size\n",
        "    \n",
        "    \n",
        "    class numpy_ndarray (BitmapConvBase):\n",
        "        \"\"\"\n",
        "        *Requires* :mod:`numpy`\n",
        "        \n",
        "        Get the bitmap as shaped NumPy array referencing the original ctypes array.\n",
        "        This converter never makes a copy of the data.\n",
        "        \n",
        "        Returns:\n",
        "            (numpy.ndarray, str): NumPy array, and colour format.\n",
        "        \"\"\"\n",
        "        \n",
        "        @staticmethod\n",
        "        def run(result, renderer_kws):\n",
        "            \n",
        "            if numpy is None:\n",
        "                raise RuntimeError(\"NumPy library needs to be installed for numpy_ndarray() converter.\")\n",
        "            \n",
        "            c_array, cl_format, (width, height) = result\n",
        "            np_array = numpy.ctypeslib.as_array(c_array)\n",
        "            np_array.shape = (height, width, len(cl_format))\n",
        "            \n",
        "            return np_array, cl_format\n",
        "    \n",
        "    # PIL stands for Python Imaging Library, and it's the original library that enabled Python to deal with images.\n",
        "    class pil_image (BitmapConvBase):\n",
        "        \"\"\"\n",
        "        *Requires* :mod:`PIL`\n",
        "        \n",
        "        Get the bitmap as PIL image.\n",
        "        \n",
        "        Parameters:\n",
        "            prefer_la (bool):\n",
        "                If :data:`True`, automatically convert ``RGBA``/``BGRA`` to ``LA`` if rendering in greyscale mode with alpha channel\n",
        "                (PDFium does not provide ``LA`` output directly).\n",
        "        Returns:\n",
        "            PIL.Image.Image: The image object.\n",
        "        \n",
        "        Hint:\n",
        "            This uses :func:`PIL.Image.frombuffer` under the hood.\n",
        "            If possible for the colour format in question, the image will reference the ctypes array. Otherwise, PIL may create a copy of the data.\n",
        "            Among the pixel formats supported by PDFium, PIL can directly work with ``RGBA``, ``RGBX`` or ``L``.\n",
        "            You may want to consider setting the rendering parameters *rev_byteorder* and *prefer_bgrx* to :data:`True` to generate natively compatible output.\n",
        "        \"\"\"\n",
        "        \n",
        "        @staticmethod\n",
        "        def run(result, renderer_kws, prefer_la=False):\n",
        "            \n",
        "            if PIL is None:\n",
        "                raise RuntimeError(\"Pillow library needs to be installed for pil_image() converter.\")\n",
        "            \n",
        "            c_array, cl_src, size = result\n",
        "            cl_dst = cl_src\n",
        "            if cl_src in BitmapStrReverseToRegular.keys():\n",
        "                cl_dst = BitmapStrReverseToRegular[cl_src]\n",
        "            \n",
        "            pil_image = PIL.Image.frombuffer(cl_dst, size, c_array, \"raw\", cl_src, 0, 1)\n",
        "            if prefer_la:\n",
        "                if renderer_kws.get(\"greyscale\", False) and cl_dst == \"RGBA\":\n",
        "                    pil_image = pil_image.convert(\"LA\")\n",
        "            \n",
        "            return pil_image\n",
        "\n",
        "\n",
        "class BitmapConvAliases:\n",
        "    \"\"\"\n",
        "    Base class containing rendering target aliases.\n",
        "    Currently retained for backwards compatibility, but may be deprecated in the future.\n",
        "    Consider using the :meth:`.PdfPage.render_to` / :meth:`.PdfDocument.render_to` APIs instead.\n",
        "    \"\"\"\n",
        "    \n",
        "    def render_to(self):\n",
        "        \"\"\" Method to be implemented by the inheriting class. \"\"\"\n",
        "        raise NotImplementedError(\"Inheriting class must provide render_to() method.\")\n",
        "    \n",
        "    def render_tobytes(self, **kwargs):\n",
        "        \"\"\" Alias for ``render_to(BitmapConv.any(bytes), ...)``. \"\"\"\n",
        "        return self.render_to(BitmapConv.any(bytes), **kwargs)\n",
        "    \n",
        "    def render_tonumpy(self, **kwargs):\n",
        "        \"\"\" Alias for ``render_to(BitmapConv.numpy_ndarray, ...)``. \"\"\"\n",
        "        return self.render_to(BitmapConv.numpy_ndarray, **kwargs)\n",
        "    \n",
        "    def render_topil(self, prefer_la=False, **kwargs):\n",
        "        \"\"\" Alias for ``render_to(BitmapConv.pil_image, ...)``. \"\"\"\n",
        "        return self.render_to(BitmapConv.pil_image(prefer_la=prefer_la), **kwargs)\n"
      ],
      "metadata": {
        "id": "vhMAa2w4UuAV"
      },
      "id": "vhMAa2w4UuAV",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}